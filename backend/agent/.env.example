############################################################
# Clinical AE Agent - Environment Example (.env)
# Copy this file to .env and fill required secrets.
# Naming convention: AE_<SUBSYSTEM>_<NAME>
############################################################

## Core
ENVIRONMENT=dev
AE_PROJECT_NAME=clinical-adverse-event-agent
# DEBUG | INFO | WARNING | ERROR
AE_LOG_LEVEL=INFO
# set false to disable writing decisions to disk
AE_PERSIST_DECISIONS=false
# if true, use async calls where supported
AE_ASYNC_PIPELINE=false
# optional explicit path to .env if not in working dir
ENV_PATH=

## Startup dependency wait (Mongo + LLM)
# if true, skip dependency readiness loop
AE_SKIP_STARTUP_WAIT=false
# 0=infinite; >0 abort startup after N failed attempts
AE_STARTUP_MAX_RETRIES=3
# delay between dependency checks
AE_STARTUP_RETRY_SECONDS=5
# optional custom text sent to LLM readiness probe
AE_LLM_STARTUP_PROMPT=health readiness probe

## MongoDB (required for retrieval; vector search aborts if unreachable)
# AWS IAM authentication(MONGODB-AWS)
AE_MONGO_AUTH_MECHANISM=
# MongoDB cluster hostname, used if AE_MONGO_AUTH_MECHANISM=MONGODB-AWS
AE_MONGO_CLUSTER_HOST=
AE_MONGO_URI=<Your_Mongo_URL_Here>        # mongodb+srv://user:mailto:pass@cluster.mongodb.net
MONGODB_DB_NAME=pharmacovigilance
MONGODB_CONTRACTS_COLLECTION=contracts
AE_MONGO_VECTOR_INDEX=contracts_vector_index
# driver serverSelectionTimeoutMS
MONGODB_SERVER_SELECTION_MS=30000
# workflow ping retries BEFORE search
AE_MONGO_CONNECT_RETRIES=3
# backoff between connection retries (ms)
AE_MONGO_CONNECT_BACKOFF_MS=500

## Azure OAuth (alternative to API key)
# AE_AZURE_OAUTH_TOKEN_URL=
# AE_AZURE_OAUTH_SCOPE=
# AE_AZURE_OAUTH_TOKEN_BUFFER_SECONDS=300
# AE_AZURE_OAUTH_CLIENT_ID=
# AE_AZURE_OAUTH_CLIENT_SECRET=

## -----------------------------------------------------------------------------
## Embedding Provider
## -----------------------------------------------------------------------------
AE_EMBED_PROVIDER=huggingface            # azure | huggingface
AE_EMBED_DIM=384                    

# Number of top embeddings/snippets to fetch per query
AE_EMBEDDING_TOP_K=6

# --- Azure OpenAI Embeddings (AE_EMBED_PROVIDER=azure) ---
AE_EMBED_DEPLOYMENT=               # Azure deployment name
AE_AZURE_API_VERSION=    # API version
AE_AZURE_ENDPOINT=                 # https://your-resource.openai.azure.com
AE_AZURE_API_KEY=                  # Not needed when using OAuth

# --- HuggingFace Embeddings (AE_EMBED_PROVIDER=huggingface) ---
AE_HUGGINGFACE_EMBED_MODEL=all-MiniLM-L6-v2
AE_HUGGINGFACE_DEVICE=cpu             # auto | cpu | cuda | mps

# Popular HuggingFace embedding models:
# - all-MiniLM-L6-v2      (384 dims, fast, good for dev)
# - all-mpnet-base-v2     (768 dims, better quality)
# - multi-qa-MiniLM-L6-cos-v1 (384 dims, optimized for Q&A)

## -----------------------------------------------------------------------------
## LLM Provider
## -----------------------------------------------------------------------------
AE_LLM_PROVIDER=groq              # azure | huggingface | groq | llama | ollama
AE_LLM_TEMPERATURE=1             # 0.0-1.0 (lower = more deterministic)
AE_LLM_MAX_TOKENS=2000              # max output tokens
AE_FORCE_JSON=true                 # force JSON output format

# --- Azure OpenAI LLM (AE_LLM_PROVIDER=azure) ---
AE_AZURE_CHAT_DEPLOYMENT=mixtral-8x7b-32768          # Azure chat deployment name

# --- Groq LLM (AE_LLM_PROVIDER=groq) ---
AE_GROQ_API_KEY=<Your_GROQ_API_Key_Here>               # Get from https://console.groq.com/keys
AE_GROQ_MODEL=llama-3.3-70b-versatile   # llama-3.3-70b-versatile | llama-3.1-70b-versatile | gemma-7b-it

# --- HuggingFace LLM (AE_LLM_PROVIDER=huggingface) ---
AE_HUGGINGFACE_LLM_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0
# AE_HUGGINGFACE_DEVICE=auto       # auto | cpu | cuda | mps
# AE_HUGGINGFACE_LOAD_4BIT=false   # enable 4-bit quantization (saves memory)
# AE_HUGGINGFACE_LOAD_8BIT=false   # enable 8-bit quantization

# Popular HuggingFace LLM models:
# - meta-llama/Llama-3.2-1B-Instruct (1B params, fast)
# - meta-llama/Llama-3.2-3B-Instruct (3B params, better quality)
# - mistralai/Mistral-7B-Instruct-v0.3 (7B params, high quality)

# --- Ollama LLM (AE_LLM_PROVIDER=llama or ollama) ---
AE_LLAMA_MODEL=llama3.2
AE_OLLAMA_BASE_URL=http://localhost:11434

## Filtering Defaults
AE_FILTER_IMPORTANT_DEFAULT=true
AE_FILTER_USE_KEYWORDS_DEFAULT=false

## PHI Redaction
# if true applies layered spaCy NER + regex masking
AE_REDACT_PHI=true

## LangSmith Observability (Optional)
LANGSMITH_TRACING_V2=true
LANGSMITH_API_KEY=<Your_LangSmith_API_Key_Here>
LANGSMITH_PROJECT=clinical-ae-agent
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
# if true, custom decision runs will be logged via langsmith client
AE_LANGSMITH_LOG_DECISIONS=false